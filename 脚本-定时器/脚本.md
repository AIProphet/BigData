# 脚本

### 一、准备

* [获得cleaner.jar](https://github.com/sunnyandgood/BigData/blob/master/脚本-定时器/cleaner.jar)

* 上传

### 二、执行脚本

* 执行a1.sh脚本

     * a1.sh脚本的内容
     
            CURRENT=`/bin/date +%Y%m%d`

            /softWare/hadoop-2.2.0/bin/hadoop jar /cleaner.jar /flume/$CURRENT /cleaned/$CURRENT

     * 执行a1.sh脚本
     
           # chmod +x a1.sh
           # ./a1.sh

* 执行a2.sh脚本

    * 创建外部分区表
    
            create external table wxkj(ip string,time string,urls string) partitioned by (logdate string) 
                                        row format delimited fields terminated by '\t' location '/cleaned';

    * a2.sh脚本的内容('-e' 表示不用启动hive直接在外部执行)
          
            CURRENT=`/bin/date +%Y%m%d`
            /softWare/apache-hive-0.13.0-bin/bin/hive -e "alter table wxkj 
                    add partition (logdate=$CURRENT) location '/cleaned/$CURRENT'"    
    
    * 执行脚本
 
           # chmod +x a1.sh
           # ./a1.sh   
    
    
            hive> select * from wxkj limit 4; 
            OK
            110.52.250.126	20130530173820	data/cache/style_1_widthauto.css?y7a	20180731
            110.52.250.126	20130530173820	source/plugin/wsh_wx/img/wsh_zk.css	20180731
            110.52.250.126	20130530173820	data/cache/style_1_forum_index.css?y7a	20180731
            110.52.250.126	20130530173820	source/plugin/wsh_wx/img/wx_jqr.gif	20180731    
* 执行a3.sh脚本

    * a3.sh脚本的内容('-e' 表示不用启动hive直接在外部执行)
            
            CURRENT=`/bin/date +%Y%m%d`
            
            /softWare/apache-hive-0.13.0-bin/bin/hive -e "select count(*) from wxkj where logdate = $CURRENT"
    
    * 执行脚本

            # chmod +x a1.sh
            # ./a3.sh
            18/07/31 11:45:13 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, 
                                                                        use mapreduce.job.reduces
            18/07/31 11:45:13 INFO Configuration.deprecation: mapred.min.split.size is deprecated. Instead, 
                                                use mapreduce.input.fileinputformat.split.minsize
            .....................................................
            Total jobs = 1
            Launching Job 1 out of 1
            Number of reduce tasks determined at compile time: 1
            In order to change the average load for a reducer (in bytes):
              set hive.exec.reducers.bytes.per.reducer=<number>
            In order to limit the maximum number of reducers:
              set hive.exec.reducers.max=<number>
            In order to set a constant number of reducers:
              set mapreduce.job.reduces=<number>
            Starting Job = job_1533054465340_0011, Tracking URL = http://hadoop03:8088/proxy/application_
                                                                    1533054465340_0011/
            Kill Command = /softWare/hadoop-2.2.0/bin/hadoop job  -kill job_1533054465340_0011
            Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
            2018-07-31 11:45:27,096 Stage-1 map = 0%,  reduce = 0%
            2018-07-31 11:45:35,860 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.43 sec
            2018-07-31 11:45:42,143 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 4.48 sec
            MapReduce Total cumulative CPU time: 4 seconds 480 msec
            Ended Job = job_1533054465340_0011
            MapReduce Jobs Launched: 
            Job 0: Map: 1  Reduce: 1   Cumulative CPU: 4.48 sec   HDFS Read: 12756871 HDFS Write: 7 SUCCESS
            Total MapReduce CPU Time Spent: 4 seconds 480 msec
            OK
            169859
            Time taken: 27.3 seconds, Fetched: 1 row(s)

* 执行a4.sh脚本

    * a4.sh脚本的内容('-e' 表示不用启动hive直接在外部执行)
    
            CURRENT=`/bin/date +%Y%m%d`

            /softWare/apache-hive-0.13.0-bin/bin/hive -e "select count(distinct ip) 
                                                from wxkj where logdate = $CURRENT"
    
    * 执行脚本
    
            # chmod +x a1.sh
            # ./a4.sh
            18/07/31 11:57:23 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. 
                                                                    Instead, use mapreduce.job.reduces
            ....................................................
            Total jobs = 1
            Launching Job 1 out of 1
            Number of reduce tasks determined at compile time: 1
            In order to change the average load for a reducer (in bytes):
              set hive.exec.reducers.bytes.per.reducer=<number>
            In order to limit the maximum number of reducers:
              set hive.exec.reducers.max=<number>
            In order to set a constant number of reducers:
              set mapreduce.job.reduces=<number>
            Starting Job = job_1533054465340_0013, Tracking URL = http://hadoop03:8088/proxy/application_
                                                                                1533054465340_0013/
            Kill Command = /softWare/hadoop-2.2.0/bin/hadoop job  -kill job_1533054465340_0013
            Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
            2018-07-31 11:57:36,443 Stage-1 map = 0%,  reduce = 0%
            2018-07-31 11:57:42,744 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 2.13 sec
            2018-07-31 11:57:49,010 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 3.18 sec
            MapReduce Total cumulative CPU time: 3 seconds 180 msec
            Ended Job = job_1533054465340_0013
            MapReduce Jobs Launched: 
            Job 0: Map: 1  Reduce: 1   Cumulative CPU: 3.18 sec   HDFS Read: 12756871 HDFS Write: 6 SUCCESS
            Total MapReduce CPU Time Spent: 3 seconds 180 msec
            OK
            10413
            Time taken: 23.223 seconds, Fetched: 1 row(s)

* 执行a5.sh脚本
    
    * a5.sh脚本的内容('-e' 表示不用启动hive直接在外部执行)
    
            /softWare/apache-hive-0.13.0-bin/bin/hive -e "select count(*) from wxkj where 
                        logdate = $CURRENT and instr(urls, 'member.php?mod=register')>0;"
    
    * 执行脚本
    
            # chmod +x a1.sh
            # ./a5.sh
            

            
* 执行a6.sh脚本
    
    * a6.sh脚本的内容('-e' 表示不用启动hive直接在外部执行)    
    
    * 执行脚本
    
            # chmod +x a1.sh
            # ./a6.sh    
    
    
     



           
           
